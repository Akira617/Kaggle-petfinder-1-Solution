{"cells":[{"cell_type":"markdown","metadata":{},"source":["## libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-01-03T02:17:12.312215Z","iopub.status.busy":"2022-01-03T02:17:12.311876Z","iopub.status.idle":"2022-01-03T02:17:18.988364Z","shell.execute_reply":"2022-01-03T02:17:18.987248Z","shell.execute_reply.started":"2022-01-03T02:17:12.312132Z"},"trusted":true},"outputs":[],"source":["import sys\n","import warnings\n","import sklearn.exceptions\n","warnings.filterwarnings(\"ignore\")\n","#general\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.svm import SVR\n","from sklearn.preprocessing import RobustScaler\n","import pickle\n","from tqdm.auto import tqdm\n","from collections import defaultdict\n","import os\n","import numpy as np\n","import pandas as pd\n","import random\n","import gc\n","import cv2\n","gc.enable()\n","import glob\n","pd.set_option('display.max_columns', None) \n","from sklearn.linear_model import RidgeCV\n","\n","# visualization\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# augmentation\n","from albumentations.pytorch import ToTensorV2\n","import albumentations as A\n","\n","# deep learning\n","import timm\n","from torch.cuda.amp import autocast, GradScaler\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR, CosineAnnealingLR, ReduceLROnPlateau, StepLR, LambdaLR\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import imageio\n","from PIL import Image\n","from tqdm.notebook import tqdm\n","tqdm.pandas()\n","\n","# metrics\n","from sklearn.metrics import mean_squared_error\n","from torch.nn import DataParallel"]},{"cell_type":"markdown","metadata":{},"source":["## Config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-03T02:17:28.195802Z","iopub.status.busy":"2022-01-03T02:17:28.195396Z","iopub.status.idle":"2022-01-03T02:17:28.210181Z","shell.execute_reply":"2022-01-03T02:17:28.209117Z","shell.execute_reply.started":"2022-01-03T02:17:28.195734Z"},"trusted":true},"outputs":[],"source":["class Config:\n","    expid = \"010923\" # 实验id\n","    num_workers = 10 # cpu线程\n","    inp_channels = 3 # input channel\n","    gpu_parallel = False # 并行gpu\n","    batch_size = 16\n","    \n","    out_features = 1 # output dim\n","    epoch_step_valid = 3 # 第n个epoch后开始valid\n","    data_dir = \"/home/xm/workspace/petfinder-pawpularity-score/\" # 数据路径\n","    output_dir = f\"/home/xm/workspace/output\" # 输出路径\n","    img_train_dir = os.path.join(data_dir, \"train\") # 训练图片路径\n","    img_test_dir = os.path.join(data_dir, \"test\")  # 测试图片路径\n","    random_seed = 42 # seed\n","\n","    n_epoch = 5 \n","    n_fold = 10 #\n","    steps_per_epoch = 31 # 每n个steps进行一次valid\n","    model_path = \"swin_large_patch4_window7_224\" # 模型，swin_large_patch4_window12_384, swin_large_patch4_window7_224 \n","    pretrained = True # 使用预训练权重\n","    im_size =  224 # image size\n","    \n","    lr = 2e-5\n","    opt_wd_non_norm_bias = 0.01 # weight decay\n","    opt_wd_norm_bias = 0 # bias weight decay\n","    opt_beta1 = 0.9 # adam\n","    opt_beta2 = 0.99 # adam\n","    opt_eps = 1e-5 # 最小lr\n","\n","    scheduler_name = \"OneCycleLR\" # 调度器\n","    reduce_lr_factor = 0.6 # 学习率减小幅度\n","    reduce_lr_patience = 1 # 学习率减小耐心值\n","    T_0 = 4 # CosineAnnealingWarmRestarts调度器周期\n","    T_max =4 # CosineAnnealingLR 调度器周期\n","    T_mult =1 # 学习率变化倍数\n","    min_lr = 1e-7 # CosineAnnealingWarmRestarts 最小学习率\n","    max_lr = 2e-5 # OneCycleLR 最大学习率\n","\n","    tta = True # calculate cv score in case TTA is executed\n","    tta_times = 4 # tta次数\n","    tta_beta = 1 / tta_times # 每次tta的权重\n","\n","    mixup = False # 是否使用mixup增强\n","    if mixup:\n","        mixup_epoch = n_epoch\n","    else:\n","        mixup_epoch = 0\n","    mixup_alpha =0.2\n","\n","    is_debug = False # 调试模式\n","    if is_debug:\n","        n_epoch = 1\n","        n_fold = 2\n","        n_sample_debug = 500\n","        tta_times = 2\n","        tta_beta = 1 / tta_times"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-03T02:17:28.213686Z","iopub.status.busy":"2022-01-03T02:17:28.213321Z","iopub.status.idle":"2022-01-03T02:17:28.504042Z","shell.execute_reply":"2022-01-03T02:17:28.503108Z","shell.execute_reply.started":"2022-01-03T02:17:28.213642Z"},"trusted":true},"outputs":[],"source":["def seed_torch(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","seed_torch(seed=Config.random_seed)\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(f'Using device: {device}')\n","\n","def divice_norm_bias(model): \n","    norm_bias_params = []\n","    non_norm_bias_params = []\n","    except_wd_layers = ['norm', '.bias']\n","    for n, p in model.named_parameters():\n","        if any([nd in n for nd in except_wd_layers]):\n","            norm_bias_params.append(p)\n","        else:\n","            non_norm_bias_params.append(p)\n","    return norm_bias_params, non_norm_bias_params\n","\n","def usr_rmse_score(output, target):\n","    y_pred = torch.sigmoid(output).cpu()\n","    y_pred = y_pred.detach().numpy()*100\n","    target = target.cpu()*100\n","    \n","    return mean_squared_error(target, y_pred, squared=False)\n","\n","def rmse_oof(_oof_df, fold=None):\n","    oof_df = _oof_df.copy()\n","    if fold is not None:\n","        oof_df = oof_df[oof_df[\"fold\"] == fold]\n","    target = oof_df['Pawpularity'].values\n","    y_pred = oof_df['pred'].values\n","    if fold is not None:\n","        print(f'fold {fold}: {mean_squared_error(target, y_pred, squared=False)}')\n","    else:\n","        overall_rmse = mean_squared_error(target, y_pred, squared=False)\n","        print(f'overall: {overall_rmse}')\n","        return overall_rmse\n","\n","class MetricMonitor:\n","    def __init__(self, float_precision=3):\n","        self.float_precision = float_precision\n","        self.reset()\n","\n","    def reset(self):\n","        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n","\n","    def update(self, metric_name, val):\n","        metric = self.metrics[metric_name]\n","\n","        metric[\"val\"] += val\n","        metric[\"count\"] += 1\n","        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n","\n","    def __str__(self):\n","        return \" | \".join(\n","            [\n","                \"{metric_name}: {avg:.{float_precision}f}\".format(\n","                    metric_name=metric_name, avg=metric[\"avg\"],\n","                    float_precision=self.float_precision\n","                )\n","                for (metric_name, metric) in self.metrics.items()\n","            ]\n","        )\n","    \n","def get_scheduler(optimizer):\n","    scheduler = None\n","    if Config.scheduler_name == 'CosineAnnealingWarmRestarts':\n","        scheduler = CosineAnnealingWarmRestarts(\n","            optimizer,\n","            T_0=Config.T_0,\n","            eta_min=Config.min_lr,\n","            last_epoch=-1\n","        )\n","    elif Config.scheduler_name == 'OneCycleLR':\n","        scheduler = OneCycleLR(\n","            optimizer,\n","            max_lr=Config.max_lr,\n","            pct_start = 0.25, # same as fastai, defaut 0.3\n","            steps_per_epoch=int(((Config.n_fold - 1) * train_df.shape[0]) / (Config.n_fold * Config.batch_size)) + 1,\n","            epochs = Config.n_epoch\n","        )\n","\n","    elif Config.scheduler_name == 'CosineAnnealingLR':\n","        scheduler = CosineAnnealingLR(\n","            optimizer,\n","            T_max=Config.T_max,\n","            eta_min=Config.min_lr,\n","            last_epoch=-1\n","        )\n","    elif Config.scheduler_name == 'ReduceOnPlateauLR':\n","        scheduler = ReduceLROnPlateau(\n","            optimizer,\n","            mode = 'min',\n","            factor=Config.reduce_lr_factor,\n","            patience=Config.reduce_lr_patience,\n","            verbose = True\n","        )\n","    return scheduler\n","\n","\n","from torch.nn.modules.loss import _WeightedLoss\n","\n","class SmoothBCEwLogits(_WeightedLoss):\n","    def __init__(self, weight = None, reduction = 'mean', smoothing = 0.0, pos_weight = None):\n","        super().__init__(weight=weight, reduction=reduction)\n","        self.smoothing = smoothing\n","        self.weight = weight\n","        self.reduction = reduction\n","        self.pos_weight = pos_weight\n","\n","    @staticmethod\n","    def _smooth(targets, n_labels, smoothing = 0.0):\n","        assert 0 <= smoothing < 1\n","        with torch.no_grad(): targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n","        return targets\n","\n","    def forward(self, inputs, targets):\n","        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1), self.smoothing)\n","        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight, pos_weight = self.pos_weight)\n","        if  self.reduction == 'sum': loss = loss.sum()\n","        elif  self.reduction == 'mean': loss = loss.mean()\n","        return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-03T02:17:28.507573Z","iopub.status.busy":"2022-01-03T02:17:28.506985Z","iopub.status.idle":"2022-01-03T02:17:28.668431Z","shell.execute_reply":"2022-01-03T02:17:28.667413Z","shell.execute_reply.started":"2022-01-03T02:17:28.507501Z"},"trusted":true},"outputs":[],"source":["def return_imgfilepath(name, folder=Config.img_train_dir):\n","    path = os.path.join(folder, f'{name}.jpg')\n","    return path\n","\n","train_df = pd.read_csv(os.path.join(Config.data_dir, 'train.csv'))\n","\n","# set image filepath\n","train_df['file_path'] = train_df['Id'].progress_apply(lambda x: return_imgfilepath(x))\n","\n","# del 27 image\n","del_list = [\"b148cbea87c3dcc65a05b15f78910715\", \"9a0238499efb15551f06ad583a6fa951\", \"e359704524fa26d6a3dcd8bfeeaedd2e\", \"5a642ecc14e9c57a05b8e010414011f2\", \"bf8501acaeeedc2a421bac3d9af58bb7\", \n","            \"01430d6ae02e79774b651175edd40842\", \"1feb99c2a4cac3f3c4f8a4510421d6f5\", \"6ae42b731c00756ddd291fa615c822a1\", \"13d215b4c71c3dc603cd13fc3ec80181\", \"3877f2981e502fe1812af38d4f511fd2\", \n","            \"5ef7ba98fc97917aec56ded5d5c2b099\", \"988b31dd48a1bc867dbc9e14d21b05f6\", \"9b3267c1652691240d78b7b3d072baf3\", \"72b33c9c368d86648b756143ab19baeb\", \"2b737750362ef6b31068c4a4194909ed\", \n","            \"b49ad3aac4296376d7520445a27726de\", \"9f5a457ce7e22eecd0992f4ea17b6107\", \"dd042410dc7f02e648162d7764b50900\", \"a9513f7f0c93e179b87c01be847b3e4c\", \"1059231cf2948216fcc2ac6afb4f8db8\", \n","            \"87c6a8f85af93b84594a36f8ffd5d6b8\", \"8ffde3ae7ab3726cff7ca28697687a42\", \"dbc47155644aeb3edd1bd39dba9b6953\", \"38426ba3cbf5484555f2b5e9504a6b03\", \"54563ff51aa70ea8c6a9325c15f55399\", \n","            \"fe47539e989df047507eaa60a16bc3fd\", \"78a02b3cb6ed38b2772215c0c0a7f78e\"]\n","\n","train_df = train_df.drop(train_df[train_df[\"Id\"].isin(del_list)].index).reset_index(drop=True)\n","\n","if Config.is_debug:\n","    train_df = train_df.sample(500).reset_index(drop = True)\n","train_df['norm_score'] = train_df['Pawpularity'] / 100\n","num_bins = int(np.floor(1+(3.3)*(np.log2(len(train_df)))))\n","train_df['bins'] = pd.cut(train_df['norm_score'], bins=num_bins, labels=False)\n","target_bins = train_df['bins']\n","print(\"num_bins:\", num_bins)\n","\n","train_df['fold'] = -1\n","skf = StratifiedKFold(n_splits = Config.n_fold, shuffle=True, random_state =Config.random_seed)\n","for i, (_, valid_index) in enumerate(skf.split(train_df.index, train_df['bins'])):\n","    train_df.iloc[valid_index, -1] = i\n","    \n","train_df['fold'] = train_df['fold'].astype('int')\n","\n","train_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-03T02:17:29.12704Z","iopub.status.busy":"2022-01-03T02:17:29.126421Z","iopub.status.idle":"2022-01-03T02:17:29.138362Z","shell.execute_reply":"2022-01-03T02:17:29.13734Z","shell.execute_reply.started":"2022-01-03T02:17:29.126994Z"},"trusted":true},"outputs":[],"source":["class PetDataset(Dataset):\n","    def __init__(self, image_filepaths, targets, transform=None):\n","        self.image_filepaths = image_filepaths\n","        self.targets = targets\n","        self.transform = transform\n","    \n","    def __len__(self):\n","        return len(self.image_filepaths)\n","\n","    def __getitem__(self, idx):\n","        image_filepath = self.image_filepaths[idx]\n","        with open(image_filepath, 'rb') as f:\n","            image = Image.open(f)\n","            image_rgb = image.convert('RGB')\n","        image = np.array(image_rgb)\n","\n","        if self.transform is not None:\n","            image = self.transform(image = image)[\"image\"]\n","        \n","        image = image / 255 # convert to 0-1\n","        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n","        target = self.targets[idx]\n","\n","        image = torch.tensor(image, dtype = torch.float)\n","        target = torch.tensor(target, dtype = torch.float)\n","        return image, target"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-03T02:17:29.142454Z","iopub.status.busy":"2022-01-03T02:17:29.142159Z","iopub.status.idle":"2022-01-03T02:17:29.159952Z","shell.execute_reply":"2022-01-03T02:17:29.15904Z","shell.execute_reply.started":"2022-01-03T02:17:29.142422Z"},"trusted":true},"outputs":[],"source":["IMAGENET_MEAN = [0.485, 0.456, 0.406]  # RGB\n","IMAGENET_STD = [0.229, 0.224, 0.225]  # RGB\n","def get_train_transforms(epoch, dim = Config.im_size):\n","    return A.Compose(\n","        [             \n","            # resize like Resize in fastai\n","            A.SmallestMaxSize(max_size=dim, p=1.0),\n","            A.RandomCrop(height=dim, width=dim, p=1.0),\n","            A.VerticalFlip(p = 0.5),\n","            A.HorizontalFlip(p = 0.5)\n","            #A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n","        ]\n","  )\n","\n","def get_inference_fixed_transforms(mode=0, dim = Config.im_size):\n","    if mode == 0: # do not original aspects, colors and angles\n","        return A.Compose([\n","                A.SmallestMaxSize(max_size=dim, p=1.0),\n","                A.CenterCrop(height=dim, width=dim, p=1.0),\n","                #A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n","            ], p=1.0)\n","    elif mode == 1:\n","        return A.Compose([\n","                A.SmallestMaxSize(max_size=dim, p=1.0),\n","                A.CenterCrop(height=dim, width=dim, p=1.0),\n","                #A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),,\n","                A.VerticalFlip(p = 1.0)\n","            ], p=1.0)    \n","    elif mode == 2:\n","        return A.Compose([\n","                A.SmallestMaxSize(max_size=dim, p=1.0),\n","                A.CenterCrop(height=dim, width=dim, p=1.0),\n","                #A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n","                A.HorizontalFlip(p = 1.0)\n","            ], p=1.0)\n","    elif mode == 3:\n","        return A.Compose([\n","                A.SmallestMaxSize(max_size=dim, p=1.0),\n","                A.CenterCrop(height=dim, width=dim, p=1.0),\n","                #A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n","                A.Transpose(p=1.0)\n","            ], p=1.0)\n","        \n","def get_inference_random_transforms(mode=0, dim = Config.im_size):\n","    if mode == 0: # do not original aspects, colors and angles\n","        return A.Compose([\n","                A.SmallestMaxSize(max_size=dim, p=1.0),\n","                A.CenterCrop(height=dim, width=dim, p=1.0),\n","                #A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n","            ], p=1.0)\n","    else:\n","        return A.Compose(\n","            [            \n","                A.SmallestMaxSize(max_size=dim, p=1.0),\n","                A.CenterCrop(height=dim, width=dim, p=1.0),\n","                A.VerticalFlip(p = 0.5),\n","                A.HorizontalFlip(p = 0.5)\n","                #A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n","            ]\n","      ) "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# train_dataset = PetDataset(\n","#     image_filepaths = train_df['file_path'].values,\n","#     targets = train_df['Pawpularity'].values /100,\n","#     transform = get_train_transforms(0)\n","# )\n","\n","# print(train_dataset[0][0].shape)\n","# train_dataset[0]"]},{"cell_type":"markdown","metadata":{},"source":["## model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-03T02:17:29.165568Z","iopub.status.busy":"2022-01-03T02:17:29.165155Z","iopub.status.idle":"2022-01-03T02:17:29.174905Z","shell.execute_reply":"2022-01-03T02:17:29.173845Z","shell.execute_reply.started":"2022-01-03T02:17:29.165505Z"},"trusted":true},"outputs":[],"source":["class PetNet(nn.Module):\n","    def __init__(\n","        self,\n","        model_name = Config.model_path,\n","        out_features = Config.out_features,\n","        inp_channels=Config.inp_channels,\n","        pretrained=Config.pretrained\n","    ):\n","        super().__init__()\n","        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels, num_classes = out_features)\n","        print(\"self.model.head.in_features:\",self.model.head.in_features)\n","        self.model.head = nn.Linear(self.model.head.in_features, 128) # 1536\n","        self.dropout1 = nn.Dropout(0.1)\n","        self.dense1 = nn.Linear(128, 64)\n","        self.relu = nn.ReLU()\n","        self.dense2 = nn.Linear(64, 1)\n","\n","    \n","    def forward(self, image):\n","        x1 = self.model(image)          # [bs, 128]\n","        x = self.dropout1(x1)           # [bs, 128]\n","        x = self.dense1(x)              # [bs, 64]\n","        x = self.relu(x)                # [bs, 64]\n","        x = self.dense2(x)              # [bs, 1]\n","        x2 = torch.cat([x, x1], dim=1)  # [bs, 129]\n","        return x, x2"]},{"cell_type":"markdown","metadata":{},"source":["## helper function"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-01-03T02:17:29.216534Z","iopub.status.busy":"2022-01-03T02:17:29.21485Z","iopub.status.idle":"2022-01-03T02:17:29.231815Z","shell.execute_reply":"2022-01-03T02:17:29.230488Z","shell.execute_reply.started":"2022-01-03T02:17:29.216496Z"},"trusted":true},"outputs":[],"source":["def valid_fn(y_valid, X_valid_paths, model, criterion, epoch):\n","    model.eval()\n","    #model = layer_freeze(model)\n","    test_targets = []\n","    test_preds = []\n","    valid_dataset = PetDataset(\n","      image_filepaths = X_valid_paths,\n","      targets = y_valid,\n","      transform = get_inference_fixed_transforms(0)\n","    )\n","    valid_loader = DataLoader(\n","      valid_dataset,\n","      batch_size = Config.batch_size,\n","      shuffle = False,\n","      num_workers = Config.num_workers,\n","      pin_memory = False\n","    )\n","    metric_monitor = MetricMonitor()\n","    stream = tqdm(valid_loader)\n","    for i, (images, target) in enumerate(stream, start = 1):\n","        images = images.to(device, non_blocking = True).float()\n","        target = target.to(device, non_blocking = True).float().view(-1, 1)\n","        with torch.no_grad():\n","            output, _ = model(images)\n","        loss = criterion(output, target)\n","        rmse_score = usr_rmse_score(output, target)\n","        metric_monitor.update('Loss', loss.item())\n","        metric_monitor.update('RMSE', rmse_score)\n","        stream.set_description(f\"Epoch: {epoch:02}. Valid. {metric_monitor}\")\n","\n","        targets = (target.detach().cpu().numpy() * 100).ravel().tolist()\n","        pred = (torch.sigmoid(output).detach().cpu().numpy() * 100).ravel().tolist()\n","\n","        test_preds.extend(pred)\n","        test_targets.extend(targets)\n","    test_preds = np.array(test_preds)\n","    test_targets = np.array(test_targets)\n","    del valid_loader, valid_dataset, target, output\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    return test_targets, test_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-03T02:17:29.234215Z","iopub.status.busy":"2022-01-03T02:17:29.233795Z","iopub.status.idle":"2022-01-03T02:17:29.252004Z","shell.execute_reply":"2022-01-03T02:17:29.25068Z","shell.execute_reply.started":"2022-01-03T02:17:29.234151Z"},"trusted":true},"outputs":[],"source":["def tta_fn(y_valid, X_valid_paths, model, criterion, epoch):\n","    model.eval()\n","    #model = layer_freeze(model)\n","    test_targets = []\n","    test_preds = []\n","    for tta_mode in range(Config.tta_times):\n","        print(f'tta mode:{tta_mode}')\n","        valid_dataset = PetDataset(\n","          image_filepaths = X_valid_paths,\n","          targets = y_valid,\n","          transform = get_inference_fixed_transforms(tta_mode)\n","        )\n","        valid_loader = DataLoader(\n","          valid_dataset,\n","          batch_size = Config.batch_size,\n","          shuffle = False,\n","          num_workers = Config.num_workers,\n","          pin_memory = False\n","        )\n","        metric_monitor = MetricMonitor()\n","        stream = tqdm(valid_loader)\n","        tta_preds = []\n","        for i, (images, target) in enumerate(stream, start = 1):\n","            images = images.to(device, non_blocking = True).float()\n","            target = target.to(device, non_blocking = True).float().view(-1, 1)\n","            with torch.no_grad():\n","                output, _ = model(images)\n","\n","            targets = (target.detach().cpu().numpy() * 100).ravel().tolist()\n","            pred = (torch.sigmoid(output).detach().cpu().numpy() * 100).ravel().tolist()\n","            loss = criterion(output, target)\n","            rmse_score = usr_rmse_score(output, target)\n","            metric_monitor.update('Loss', loss.item())\n","            metric_monitor.update('RMSE', rmse_score)\n","            stream.set_description(f\"Epoch: {epoch:02}. Valid. {metric_monitor}\")\n","\n","            tta_preds.extend(pred)\n","            if tta_mode == 0:\n","                test_targets.extend(targets)\n","        test_preds.append(tta_preds)\n","    test_preds = np.array(test_preds)\n","    # default preds * tta_beta + aug_preds mean * ( 1 - tta_beta)\n","    #print(test_preds.shape)\n","    final_preds = Config.tta_beta * test_preds[0] + ( 1 - Config.tta_beta) * np.mean(test_preds[1:], axis =0)\n","    test_targets = np.array(test_targets)\n","    del valid_loader, valid_dataset, target, output\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    return test_targets, final_preds"]},{"cell_type":"markdown","metadata":{},"source":["## training loop"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-03T02:17:29.254534Z","iopub.status.busy":"2022-01-03T02:17:29.254152Z","iopub.status.idle":"2022-01-03T02:17:29.283796Z","shell.execute_reply":"2022-01-03T02:17:29.282567Z","shell.execute_reply.started":"2022-01-03T02:17:29.254455Z"},"trusted":true},"outputs":[],"source":["new_filename_list = []\n","def training_loop(filepaths, targets):\n","    oof_df = pd.DataFrame()\n","    for i_fold, (train_idx, valid_idx) in enumerate(skf.split(filepaths, target_bins)):\n","        print(f'=== fold {i_fold}: training ===')\n","        \"\"\"\n","        separate train/valid data \n","        \"\"\"\n","        X_train_paths = filepaths[train_idx]\n","        y_train = targets[train_idx]\n","        X_valid_paths = filepaths[valid_idx]\n","        y_valid = targets[valid_idx]\n","        valid_ids = ids[valid_idx]\n","\n","        train_dataset = PetDataset(image_filepaths = X_train_paths, targets = y_train, transform = get_train_transforms(0))\n","        train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True, num_workers=Config.num_workers, pin_memory=False)\n","        \"\"\"\n","        instantiate model, cost function and optimizer\n","        \"\"\"\n","        model = PetNet()\n","\n","        if Config.gpu_parallel: # 根据配置决定是否启用多GPU\n","            num_gpu = torch.cuda.device_count()\n","            model = DataParallel(model, device_ids=range(num_gpu))\n","\n","        model = model.to(device)\n","        criterion = SmoothBCEwLogits(smoothing=0.10) # nn.BCEWithLogitsLoss() # SmoothBCEwLogits(smoothing=0.10)  \n","        norm_bias_params, non_norm_bias_params = divice_norm_bias(model)\n","        #print(f\"norm bias params: {len(norm_bias_params)}, non norm bias params: {len(non_norm_bias_params)}\")\n","        optimizer = torch.optim.AdamW(\n","            [\n","              {'params': norm_bias_params, 'weight_decay': Config.opt_wd_norm_bias},\n","              {'params': non_norm_bias_params, 'weight_decay': Config.opt_wd_non_norm_bias},\n","          ],\n","          betas=(Config.opt_beta1, Config.opt_beta2),\n","          eps=Config.opt_eps,\n","          lr = Config.lr,\n","          amsgrad = False\n","        )\n","        scheduler = get_scheduler(optimizer)\n","        \"\"\"\n","        train / valid loop\n","        \"\"\"\n","        best_rmse = np.inf\n","        scaler = GradScaler()\n","        for epoch in range(1, Config.n_epoch + 1):\n","            print(f'=== fold:{i_fold} epoch: {epoch}: training ===')\n","            \n","            metric_monitor = MetricMonitor()\n","            stream = tqdm(train_loader)\n","\n","            for batch_idx, (images, target) in enumerate(stream, start = 1):\n","            #for batch_idx, (images, target) in enumerate(train_loader):\n","                model.train()\n","                #train_fn(train_loader, model, criterion, optimizer, epoch, params, scheduler)\n","                if Config.mixup_epoch >= epoch:\n","                    images, target_a, target_b, lam = mixup_data(images, target.view(-1 ,1))\n","                    images = images.to(device, dtype = torch.float)\n","                    target_a = target_a.to(device, dtype = torch.float)\n","                    target_b = target_b.to(device, dtype = torch.float)\n","                else:\n","                    images = images.to(device, non_blocking = True).float()\n","                    target = target.to(device, non_blocking = True).float().view(-1, 1)\n","                optimizer.zero_grad()\n","                with autocast(): # mixed precision\n","                    output, _ = model(images)\n","                    loss = mixup_criterion(criterion, output, target_a, target_b, lam) if Config.mixup_epoch >= epoch else criterion(output, target)\n","\n","                rmse_score = usr_rmse_score(output, target)\n","                metric_monitor.update('Loss', loss.item())\n","                metric_monitor.update('RMSE', rmse_score)\n","                stream.set_description(f'Epoch: {epoch:02}. Train. {metric_monitor}, Lr:{optimizer.param_groups[0][\"lr\"]:.1e}')\n","                scaler.scale(loss).backward()\n","                scaler.step(optimizer)\n","                scaler.update()\n","\n","                if (scheduler is not None) & (Config.scheduler_name != 'ReduceOnPlateauLR') :\n","                    scheduler.step()\n","            \n","                if ( ( ( batch_idx % Config.steps_per_epoch == 0) & (epoch >= Config.epoch_step_valid) ) | ( batch_idx == len(train_loader) ) ):\n","                    valid_targets, preds = valid_fn(y_valid, X_valid_paths, model, criterion, epoch)\n","                    valid_rmse = round(mean_squared_error(valid_targets, preds, squared=False), 3)\n","                    print(f'epoch: {epoch}, batch: {batch_idx}/{len(train_loader)}, valid rmse: {valid_rmse}')\n","                    if Config.scheduler_name == 'ReduceOnPlateauLR':\n","                        scheduler.step(valid_rmse)\n","\n","                    if valid_rmse < best_rmse:\n","                        best_rmse = valid_rmse\n","                        model_name = Config.model_path\n","                        torch.save(model.state_dict(), f'{Config.output_dir}/{Config.expid}_{model_name}_fold{i_fold}.pth')\n","                        print(\"saved model.\")\n","                        _oof_df = pd.DataFrame(data={'Id': valid_ids, 'pred':preds, 'fold': i_fold, 'Pawpularity':valid_targets}, index=valid_idx)\n","        \n","        old_filename = f'{Config.output_dir}/{Config.expid}_{model_name}_fold{i_fold}.pth'\n","        new_filename = f'{Config.output_dir}/{Config.expid}_{model_name}_fold{i_fold}_cv{best_rmse*1000:.0f}.pth'\n","        new_filename_list.append(new_filename)\n","        os.rename(old_filename,  new_filename)\n","\n","        del model, output, train_loader, train_dataset\n","        gc.collect()\n","        \n","        torch.cuda.empty_cache()\n","        oof_df = pd.concat([oof_df, _oof_df])\n","    return oof_df.sort_values('Id')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-03T02:17:29.286395Z","iopub.status.busy":"2022-01-03T02:17:29.285647Z","iopub.status.idle":"2022-01-03T02:17:29.300378Z","shell.execute_reply":"2022-01-03T02:17:29.299279Z","shell.execute_reply.started":"2022-01-03T02:17:29.286348Z"},"trusted":true},"outputs":[],"source":["ids = train_df['Id'].values\n","filepaths = train_df['file_path'].values\n","targets = train_df['Pawpularity'].values/100\n","oof_df = training_loop(filepaths, targets)"]},{"cell_type":"markdown","metadata":{},"source":["## TTA"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-03T02:18:40.58389Z","iopub.status.busy":"2022-01-03T02:18:40.5823Z","iopub.status.idle":"2022-01-03T02:18:40.595452Z","shell.execute_reply":"2022-01-03T02:18:40.594441Z","shell.execute_reply.started":"2022-01-03T02:18:40.583841Z"},"trusted":true},"outputs":[],"source":["def tta_loop(filepaths, targets):\n","    oof_df = pd.DataFrame()\n","    for i_fold, (train_idx, valid_idx) in enumerate(skf.split(filepaths, target_bins)):\n","        print(f'=== fold {i_fold}: validation ===')\n","        \"\"\"\n","        separate valid data \n","        \"\"\"\n","        X_valid_paths = filepaths[valid_idx]\n","        y_valid = targets[valid_idx]\n","        valid_ids = ids[valid_idx]\n","        \"\"\"\n","        instantiate model, cost function and optimizer\n","        \"\"\"\n","        model = PetNet()\n","        model_name = new_filename_list[i_fold] # f'{Config.output_dir}/{Config.expid}_{model_name}_fold{i_fold}.pth'\n","        model.load_state_dict(torch.load(model_name))\n","        model = model.to(device)\n","        criterion = nn.BCEWithLogitsLoss()\n","        epoch = 0\n","        valid_targets, preds = tta_fn(y_valid, X_valid_paths, model, criterion, epoch)\n","        valid_rmse = round(mean_squared_error(valid_targets, preds, squared=False), 3)\n","        _oof_df = pd.DataFrame(data={'Id': valid_ids, 'pred':preds, 'fold': i_fold, 'Pawpularity':valid_targets}, index=valid_idx)\n","        del model\n","        gc.collect()\n","        \n","        torch.cuda.empty_cache()\n","        oof_df = pd.concat([oof_df, _oof_df])\n","    return oof_df.sort_values('Id')"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-01-03T02:18:40.597746Z","iopub.status.busy":"2022-01-03T02:18:40.597413Z","iopub.status.idle":"2022-01-03T02:19:21.296827Z","shell.execute_reply":"2022-01-03T02:19:21.295796Z","shell.execute_reply.started":"2022-01-03T02:18:40.597678Z"},"trusted":true},"outputs":[],"source":["if Config.tta:\n","    oof_tta_df = tta_loop(filepaths, targets)"]},{"cell_type":"markdown","metadata":{},"source":["## cv score"]},{"cell_type":"markdown","metadata":{},"source":["### without TTA"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-03T02:19:21.29921Z","iopub.status.busy":"2022-01-03T02:19:21.298854Z","iopub.status.idle":"2022-01-03T02:19:21.354191Z","shell.execute_reply":"2022-01-03T02:19:21.353063Z","shell.execute_reply.started":"2022-01-03T02:19:21.299162Z"},"trusted":true},"outputs":[],"source":["# no TTA \n","for i in range(Config.n_fold):\n","    rmse_oof(oof_df, i) \n","oof_df_overall_rmse = rmse_oof(oof_df)\n","oof_df.to_csv(f'{Config.output_dir}/{Config.expid}_oof_{oof_df_overall_rmse*1000:.0f}.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.hist(oof_df['Pawpularity'].values, alpha = 0.4, color = 'b', label = 'target', bins = 50)\n","pred_bins = int((np.max(oof_df['pred'].values) - np.min(oof_df['pred'].values)) //2)\n","plt.hist(oof_df['pred'].values, alpha = 0.4, color = 'g', label = 'prediction', bins = pred_bins)\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### with TTA"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-03T02:19:21.770896Z","iopub.status.busy":"2022-01-03T02:19:21.76951Z","iopub.status.idle":"2022-01-03T02:19:21.790305Z","shell.execute_reply":"2022-01-03T02:19:21.78928Z","shell.execute_reply.started":"2022-01-03T02:19:21.770834Z"},"trusted":true},"outputs":[],"source":["# with TTA\n","for i in range(Config.n_fold):\n","    rmse_oof(oof_tta_df, i)\n","oof_tta_df_overall_rmse = rmse_oof(oof_tta_df)\n","oof_tta_df.to_csv(f'{Config.output_dir}/{Config.expid}_oof_tta_{oof_tta_df_overall_rmse*1000:.0f}.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.hist(oof_tta_df['Pawpularity'].values, alpha = 0.4, color = 'b', label = 'target', bins = 50)\n","tta_pred_bins = int((np.max(oof_tta_df['pred'].values) - np.min(oof_tta_df['pred'].values)) //2)\n","plt.hist(oof_df['pred'].values, alpha = 0.4, color = 'g', label = 'prediction', bins = tta_pred_bins)\n","plt.legend()\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":4}
